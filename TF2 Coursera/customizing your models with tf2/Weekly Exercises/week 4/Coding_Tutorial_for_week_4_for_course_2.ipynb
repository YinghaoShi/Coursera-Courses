{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Coding Tutorial for week 4 for course 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mYDtQj5cpZAx",
        "22ObAZ7ppZAx"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEffYyrWpZAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7921936e-544c-4750-c1af-8c6f72a7d22f"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7im0rUrYpZAx"
      },
      "source": [
        "# Model subclassing and custom training loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myXVAqIPpZAx"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Model subclassing](#coding_tutorial_1)\n",
        " #### [2. Custom layers](#coding_tutorial_2)\n",
        " #### [3. Automatic differentiation](#coding_tutorial_3)\n",
        " #### [4. Custom training loops](#coding_tutorial_4)\n",
        " #### [5. tf.function decorator](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYDtQj5cpZAx"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## Model subclassing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uJxSTBepZAx"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Softmax, concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8wsTYtdpZAx"
      },
      "source": [
        "#### Create a simple model using the model subclassing API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t3bbWkipZAx"
      },
      "source": [
        "# Build the model\n",
        "\n",
        "# class MyModel(Model):\n",
        "    \n",
        "#     def __init__(self):\n",
        "#         super(MyModel,self).__init__()\n",
        "#         self.Dense_1=Dense(64,activation='relu')\n",
        "#         self.Dense_2=Dense(10)\n",
        "        \n",
        "#     def call(self,inputs):\n",
        "#         x=self.Dense_1(inputs)\n",
        "#         return self.Dense_2(x)\n",
        "\n",
        "\n",
        "\n",
        "# class MyModel(Model):\n",
        "    \n",
        "#     def __init__(self):\n",
        "#         super(MyModel,self).__init__()\n",
        "#         self.Dense_1=Dense(64,activation='relu')\n",
        "#         self.Dense_2=Dense(10)\n",
        "#         self.dropout=Dropout(0.4)\n",
        "        \n",
        "#     def call(self,inputs,training=True):\n",
        "#         x=self.Dense_1(inputs)\n",
        "#         if training:\n",
        "#             x=self.dropout(x)\n",
        "#         return self.Dense_2(x)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "class MyModel(Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MyModel,self).__init__()\n",
        "        self.Dense_1=Dense(64,activation='relu')\n",
        "        self.Dense_2=Dense(10)\n",
        "        self.Dense_3=Dense(5)\n",
        "        self.softmax=Softmax()\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        x=self.Dense_1(inputs)\n",
        "        y1=self.Dense_2(inputs)\n",
        "        y2=self.Dense_3(y1)\n",
        "        concat=concatenate([x,y2])\n",
        "        return self.softmax(concat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51vHNR_1pZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c8d1e7-76af-4b42-9f06-1332fde0e6d9"
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "model=MyModel()\n",
        "model(tf.random.uniform([1,10]))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  704       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  110       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  55        \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            multiple                  0         \n",
            "=================================================================\n",
            "Total params: 869\n",
            "Trainable params: 869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ObAZ7ppZAx"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdtPg6t5pZAx"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k776r5DepZAx"
      },
      "source": [
        "#### Create custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3RMXfNUpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e01c8d5-ab0c-4db4-d177-222af8ca31a3"
      },
      "source": [
        "# Create a custom layer\n",
        "class MyLayer(Layer):\n",
        "    \n",
        "    def __init__(self,units,input_dim):\n",
        "        super(MyLayer,self).__init__()\n",
        "        self.w=self.add_weight(shape=(input_dim,units),\n",
        "                              initializer='random_normal')\n",
        "        self.b=self.add_weight(shape=(units,),\n",
        "                              initializer='zeros')\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        return tf.matmul(inputs,self.w)+self.b\n",
        "\n",
        "dense_layer=MyLayer(3,5)\n",
        "x=tf.ones((1,5))\n",
        "print(dense_layer(x))\n",
        "print(dense_layer.weights)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[ 0.24427392 -0.0462581  -0.17798297]], shape=(1, 3), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
            "array([[ 0.05310891, -0.05938276, -0.11238197],\n",
            "       [ 0.08103501, -0.01979903, -0.08205581],\n",
            "       [ 0.00843777, -0.02160801,  0.04323856],\n",
            "       [ 0.06966203,  0.00538236, -0.01045446],\n",
            "       [ 0.0320302 ,  0.04914935, -0.0163293 ]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4lTqqKApZAx"
      },
      "source": [
        "# Specify trainable weights\n",
        "\n",
        "\n",
        "# class MyLayer(Layer):\n",
        "    \n",
        "#     def __init__(self,units,input_dim):\n",
        "#         super(MyLayer,self).__init__()\n",
        "#         self.w=self.add_weight(shape=(input_dim,units),\n",
        "#                               initializer='random_normal')\n",
        "#         self.b=self.add_weight(shape=(units,),\n",
        "#                               initializer='zeros')\n",
        "        \n",
        "#     def call(self,inputs):\n",
        "#         return tf.matmul(inputs,self.w)+self.b\n",
        "\n",
        "# dense_layer=MyLayer(3,5)\n",
        "# 此时trainable weights数量为2\n",
        "\n",
        "\n",
        "class MyLayer(Layer):\n",
        "    \n",
        "    def __init__(self,units,input_dim):\n",
        "        super(MyLayer,self).__init__()\n",
        "        self.w=self.add_weight(shape=(input_dim,units),\n",
        "                              initializer='random_normal',\n",
        "                              trainable=False)\n",
        "        self.b=self.add_weight(shape=(units,),\n",
        "                              initializer='zeros',\n",
        "                              trainable=False)\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        return tf.matmul(inputs,self.w)+self.b\n",
        "\n",
        "dense_layer=MyLayer(3,5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjH3-h3lpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b63648b-3681-4be3-8ae6-e17611f67e59"
      },
      "source": [
        "print('trainable weights:', len(dense_layer.trainable_weights))\n",
        "print('non-trainable weights:', len(dense_layer.non_trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainable weights: 0\n",
            "non-trainable weights: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWHNP1A0pZAx"
      },
      "source": [
        "# Create a custom layer to accumulate means of output values\n",
        "class MyLayerMean(Layer):\n",
        "    \n",
        "    def __init__(self,units,input_dim):\n",
        "        super(MyLayerMean,self).__init__()\n",
        "        self.w=self.add_weight(shape=(input_dim,units),\n",
        "                              initializer='random_normal')\n",
        "        self.b=self.add_weight(shape=(units,),\n",
        "                              initializer='zeros')\n",
        "        self.sum_activation=tf.Variable(initial_value=tf.zeros(units,),\n",
        "                                       trainable=False)\n",
        "        self.number_call=tf.Variable(initial_value=0,\n",
        "                                       trainable=False)  \n",
        "        \n",
        "        \n",
        "    def call(self,inputs):\n",
        "        activation=tf.matmul(inputs,self.w)+self.b\n",
        "        self.sum_activation.assign_add(tf.reduce_sum(activation,axis=0))\n",
        "        self.number_call.assign_add(inputs.shape[0])\n",
        "        return activation, self.sum_activation/tf.cast(self.number_call,tf.float32)\n",
        "    \n",
        "dense_layer=MyLayerMean(3,5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFQDKJ6TpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ddfda0-406f-4395-e951-21539df66af3"
      },
      "source": [
        "# Test the layer\n",
        "\n",
        "y, activation_means = dense_layer(tf.ones((1, 5)))\n",
        "print(activation_means.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.02114812  0.22757481 -0.01501242]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geh02MuApZAx"
      },
      "source": [
        "# Create a Dropout layer as a custom layer\n",
        "\n",
        "class MyDropout(Layer):\n",
        "\n",
        "    def __init__(self, rate):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass for dropout layer\n",
        "        return tf.nn.dropout(inputs,rate=self.rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XDQ01crpZAx"
      },
      "source": [
        "#### Implement the custom layers into a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAnzVdtgpZAx"
      },
      "source": [
        "# Build the model using custom layers with the model subclassing API\n",
        "\n",
        "class MyModel(Model):\n",
        "\n",
        "    def __init__(self, units_1, input_dim_1, units_2, units_3):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer_1=MyLayer(units_1,input_dim_1)\n",
        "        self.dropout_1=MyDropout(0.5)\n",
        "        self.layer_2=MyLayer(units_2,units_1)\n",
        "        self.dropout_2=MyDropout(0.5)\n",
        "        self.layer_3=MyLayer(units_3,units_2)\n",
        "        self.softmax=Softmax()\n",
        "            \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass\n",
        "        x=self.layer_1(inputs)\n",
        "        x=tf.nn.relu(x)\n",
        "        x=self.dropout_1(x)\n",
        "        x=self.layer_2(x)\n",
        "        x=self.dropout_2(x)\n",
        "        x=self.layer_3(x)\n",
        "        return self.softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1GMiixzpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc059f0c-d535-4691-93f1-48f44afb56ae"
      },
      "source": [
        "# Instantiate a model object\n",
        "\n",
        "model = MyModel(64,10000,64,46)\n",
        "print(model(tf.ones((1, 10000))))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.00136266 0.01201757 0.05110296 0.01529321 0.0075237  0.01051984\n",
            "  0.0024334  0.09992979 0.01099001 0.00959984 0.0011579  0.00233477\n",
            "  0.01038214 0.00149689 0.00050966 0.02542372 0.07989525 0.01177404\n",
            "  0.00255131 0.00035719 0.03099341 0.01905901 0.00518561 0.00601773\n",
            "  0.00211688 0.00896639 0.00099622 0.0087784  0.11324281 0.00320092\n",
            "  0.10475652 0.00045398 0.01994743 0.00607938 0.17544156 0.02121501\n",
            "  0.01762415 0.01073785 0.02027708 0.00068772 0.00128848 0.01287215\n",
            "  0.03559376 0.00559715 0.0092152  0.00299927]], shape=(1, 46), dtype=float32)\n",
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "my_layer_2 (MyLayer)         multiple                  640064    \n",
            "_________________________________________________________________\n",
            "my_dropout (MyDropout)       multiple                  0         \n",
            "_________________________________________________________________\n",
            "my_layer_3 (MyLayer)         multiple                  4160      \n",
            "_________________________________________________________________\n",
            "my_dropout_1 (MyDropout)     multiple                  0         \n",
            "_________________________________________________________________\n",
            "my_layer_4 (MyLayer)         multiple                  2990      \n",
            "_________________________________________________________________\n",
            "softmax_1 (Softmax)          multiple                  0         \n",
            "=================================================================\n",
            "Total params: 647,214\n",
            "Trainable params: 0\n",
            "Non-trainable params: 647,214\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg0CEduzpZAx"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## Automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffg4D7M_pZAx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugrpXCv9pZAx"
      },
      "source": [
        "#### Create synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5fvF93dpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "cc780e53-d10f-4c4c-ea89-afb7262556f8"
      },
      "source": [
        "# Create data from a noise contaminated linear model\n",
        "\n",
        "def MakeNoisyData(m, b, n=20):\n",
        "    x = tf.random.uniform(shape=(n,))\n",
        "    noise = tf.random.normal(shape=(len(x),), stddev=0.1)\n",
        "    y = m * x + b + noise\n",
        "    return x, y\n",
        "\n",
        "m=1\n",
        "b=2\n",
        "x_train, y_train = MakeNoisyData(m,b)\n",
        "plt.plot(x_train, y_train, 'b.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7b7f85f860>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4jKxngDpZAx"
      },
      "source": [
        "#### Define a linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5pSB57cpZAx"
      },
      "source": [
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgLgik0TpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e754b3-bca3-4a4c-c9c8-745e4afa8f92"
      },
      "source": [
        "# Build a custom layer for the linear regression model\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LinearLayer,self).__init__()\n",
        "        self.m=self.add_weight(shape=(1,), initializer='random_normal')\n",
        "        self.b=self.add_weight(shape=(1,), initializer='zeros')\n",
        "    \n",
        "    def call(self,inputs):\n",
        "        return self.m*inputs+self.b\n",
        "\n",
        "linear_regression=LinearLayer()\n",
        "\n",
        "print(linear_regression(x_train))\n",
        "print(linear_regression.weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[0.05274564 0.02250641 0.08232226 0.03510062 0.09884682 0.06875732\n",
            " 0.06860486 0.1135046  0.09943464 0.01134074 0.01793334 0.00992438\n",
            " 0.00185378 0.08695751 0.07078692 0.01999266 0.00187572 0.05696702\n",
            " 0.10304716 0.08041999], shape=(20,), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.11984783], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCA4F0R2pZAx"
      },
      "source": [
        "#### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8dTqS7IpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6f24c7-cbcb-43a0-dbde-a24e91ff37c9"
      },
      "source": [
        "# Define the mean squared error loss function\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) \n",
        "\n",
        "starting_loss = SquaredError(linear_regression(x_train), y_train)\n",
        "print(\"Starting loss\", starting_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting loss 5.759557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoDOyQ0epZAx"
      },
      "source": [
        "#### Train and plot the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYNZWo7TpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79756d23-5185-417f-8b17-f3b1627f9a06"
      },
      "source": [
        "# Implement a gradient descent training loop for the linear regression model\n",
        "learning_rate=0.25\n",
        "steps=25\n",
        "\n",
        "for i in range(steps):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions=linear_regression(x_train)\n",
        "        loss=SquaredError(predictions, y_train)\n",
        "\n",
        "    gradients=tape.gradient(loss, linear_regression.trainable_variables)\n",
        "\n",
        "    linear_regression.m.assign_sub(learning_rate*gradients[0])\n",
        "    linear_regression.b.assign_sub(learning_rate*gradients[1])\n",
        "\n",
        "    print(f\"Step {i}: Loss {loss.numpy()}\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0: Loss 5.759556770324707\n",
            "Step 1: Loss 0.8658536076545715\n",
            "Step 2: Loss 0.13806983828544617\n",
            "Step 3: Loss 0.029752647504210472\n",
            "Step 4: Loss 0.013555251061916351\n",
            "Step 5: Loss 0.011062433943152428\n",
            "Step 6: Loss 0.010613596066832542\n",
            "Step 7: Loss 0.010474415495991707\n",
            "Step 8: Loss 0.010386578738689423\n",
            "Step 9: Loss 0.01031128503382206\n",
            "Step 10: Loss 0.01024241279810667\n",
            "Step 11: Loss 0.010178713127970695\n",
            "Step 12: Loss 0.010119687765836716\n",
            "Step 13: Loss 0.010064988397061825\n",
            "Step 14: Loss 0.010014289990067482\n",
            "Step 15: Loss 0.009967295452952385\n",
            "Step 16: Loss 0.009923744015395641\n",
            "Step 17: Loss 0.009883375838398933\n",
            "Step 18: Loss 0.009845959022641182\n",
            "Step 19: Loss 0.009811284020543098\n",
            "Step 20: Loss 0.009779141284525394\n",
            "Step 21: Loss 0.009749351069331169\n",
            "Step 22: Loss 0.009721743874251842\n",
            "Step 23: Loss 0.009696153923869133\n",
            "Step 24: Loss 0.009672428481280804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kymv-XkgpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "9dd4e63d-d89e-4322-8137-a7c73f5bd258"
      },
      "source": [
        "# Plot the learned regression model\n",
        "\n",
        "print(\"m:{},  trained m:{}\".format(m,linear_regression.m.numpy()))\n",
        "print(\"b:{},  trained b:{}\".format(b,linear_regression.b.numpy()))\n",
        "\n",
        "plt.plot(x_train, y_train, 'b.')\n",
        "\n",
        "x_linear_regression=np.linspace(min(x_train), max(x_train),50)\n",
        "plt.plot(x_linear_regression, linear_regression.m*x_linear_regression+linear_regression.b, 'r.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m:1,  trained m:[0.9960359]\n",
            "b:2,  trained b:[1.9798404]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7b7e8cbc18>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y7xZ0MdpZAx"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## Custom training loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33uxdkV4pZAx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt37qfe9pZAx"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJp-OOBMpZAx"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD5XsvnSpZAx"
      },
      "source": [
        "# Define the custom layers and model\n",
        "\n",
        "class MyLayer(Layer):\n",
        "    \n",
        "    def __init__(self,units):\n",
        "        super(MyLayer,self).__init__()\n",
        "        self.units=units\n",
        "        \n",
        "    \n",
        "    def build(self,input_shape):\n",
        "        self.w=self.add_weight(shape=(input_shape[-1],self.units),\n",
        "                              initializer='random_normal',\n",
        "                               name='kernel')\n",
        "        self.b=self.add_weight(shape=(self.units,),\n",
        "                              initializer='zeros',\n",
        "                               name='bias')\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        return tf.matmul(inputs,self.w)+self.b\n",
        "\n",
        "\n",
        "class MyDropout(Layer):\n",
        "\n",
        "    def __init__(self, rate):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass for dropout layer\n",
        "        return tf.nn.dropout(inputs,rate=self.rate)\n",
        "\n",
        "\n",
        "\n",
        "class MyModel(Model):\n",
        "\n",
        "    def __init__(self, units_1, units_2, units_3):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer_1=MyLayer(units_1)\n",
        "        self.dropout_1=MyDropout(0.5)\n",
        "        self.layer_2=MyLayer(units_2)\n",
        "        self.dropout_2=MyDropout(0.5)\n",
        "        self.layer_3=MyLayer(units_3)\n",
        "        self.softmax=Softmax()\n",
        "            \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass\n",
        "        x=self.layer_1(inputs)\n",
        "        x=tf.nn.relu(x)\n",
        "        x=self.dropout_1(x)\n",
        "        x=self.layer_2(x)\n",
        "        x=self.dropout_2(x)\n",
        "        x=self.layer_3(x)\n",
        "        return self.softmax(x)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alPLnfLj5znJ",
        "outputId": "b2aaa22a-597d-4e17-9a0b-6eb2d3d21197"
      },
      "source": [
        "model=MyModel(64,64,46)\n",
        "print(model(tf.ones((1,10000))))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.01298426 0.03139694 0.01305586 0.14666884 0.02541926 0.07128341\n",
            "  0.0052335  0.02028768 0.017014   0.01359899 0.00537334 0.0183269\n",
            "  0.01809532 0.02808102 0.0150106  0.00661893 0.03418251 0.01191718\n",
            "  0.02825511 0.02155304 0.00180324 0.03488503 0.0101872  0.00714525\n",
            "  0.01161394 0.00881161 0.01351999 0.05706073 0.01281504 0.04579319\n",
            "  0.0063702  0.02710973 0.00780887 0.01454305 0.02128719 0.02561211\n",
            "  0.0252765  0.00773866 0.01585264 0.01457291 0.0234488  0.01326172\n",
            "  0.01479688 0.01010544 0.01166715 0.01255627]], shape=(1, 46), dtype=float32)\n",
            "Model: \"my_model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "my_layer (MyLayer)           multiple                  640064    \n",
            "_________________________________________________________________\n",
            "my_dropout (MyDropout)       multiple                  0         \n",
            "_________________________________________________________________\n",
            "my_layer_1 (MyLayer)         multiple                  4160      \n",
            "_________________________________________________________________\n",
            "my_dropout_1 (MyDropout)     multiple                  0         \n",
            "_________________________________________________________________\n",
            "my_layer_2 (MyLayer)         multiple                  2990      \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            multiple                  0         \n",
            "=================================================================\n",
            "Total params: 647,214\n",
            "Trainable params: 647,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-X44qFGpZAx"
      },
      "source": [
        "#### Load the reuters dataset and define the class_names "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lne5C_lhpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb7a46b-0f48-4ba8-960c-59192862e353"
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "\n",
        "class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
        "   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
        "   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
        "   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
        "   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpY1uJ8tpZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7808b2f-e685-4026-ef00-4a9173bdbd63"
      },
      "source": [
        "# Print the class of the first sample\n",
        "\n",
        "print(\"Label: {}\".format(class_names[train_labels[0]]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: earn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T_AWnXLpZAx"
      },
      "source": [
        "#### Get the dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_9tsHu0pZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39763e81-c006-4170-ebc4-2a695437f6c6"
      },
      "source": [
        "# Load the Reuters word index\n",
        "\n",
        "word_to_index = reuters.get_word_index()\n",
        "\n",
        "invert_word_index = dict([(value, key) for (key, value) in word_to_index.items()])\n",
        "text_news = ' '.join([invert_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvLOOGbPpZAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f0eae6-6a93-4292-e8ed-ff387a34e4bc"
      },
      "source": [
        "# Print the first data example sentence\n",
        "print(text_news)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCnNPhM1pZAy"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GeutdiopZAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7472dbd3-9ebb-434d-ca36-14c2bd9cb9a9"
      },
      "source": [
        "# Define a function that encodes the data into a 'bag of words' representation\n",
        "\n",
        "def bag_of_words(text_samples, elements=10000):\n",
        "    output = np.zeros((len(text_samples), elements))\n",
        "    for i, word in enumerate(text_samples):\n",
        "        output[i, word] = 1.\n",
        "    return output\n",
        "\n",
        "x_train = bag_of_words(train_data)\n",
        "x_test = bag_of_words(test_data)\n",
        "\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train: (8982, 10000)\n",
            "Shape of x_test: (2246, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BWkGZ2hpZAy"
      },
      "source": [
        "#### Define the loss function and optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCc2Y4vBpZAy"
      },
      "source": [
        "# Define the categorical cross entropy loss and Adam optimizer\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "def loss(model, x, y, wd):\n",
        "    kernel_variables = []\n",
        "    for l in model.layers:\n",
        "        for w in l.weights:\n",
        "            if 'kernel' in w.name:\n",
        "                kernel_variables.append(w)\n",
        "    wd_penalty = wd * tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])\n",
        "    y_ = model(x)\n",
        "    return loss_object(y_true=y, y_pred=y_) + wd_penalty\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-0iMwmRpZAy"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFEuP54gpZAy"
      },
      "source": [
        "# Define a function to compute the forward and backward pass\n",
        "\n",
        "def grad(model, inputs, targets, wd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets, wd)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW7OFhqvpZAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af4f14d-debb-4201-ac04-b2f8d452975d"
      },
      "source": [
        "# Implement the training loop\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
        "train_dataset=train_dataset.batch(32)\n",
        "\n",
        "#keep the results for plotting\n",
        "train_loss_results=[]\n",
        "train_accuracy_results=[]\n",
        "\n",
        "num_epochs=10\n",
        "weight_decay=0.005\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss_avg=tf.keras.metrics.Mean()\n",
        "    epcoh_accuracy=tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    #training loop\n",
        "    for x,y in train_dataset:\n",
        "        #opimize the model\n",
        "        loss_value, grads=grad(model,x,y,weight_decay)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        #compute current loss\n",
        "        epoch_loss_avg(loss_value)\n",
        "        #compute predicted label to actual label\n",
        "        epcoh_accuracy(to_categorical(y),model(x))\n",
        "\n",
        "    #end epoch\n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epcoh_accuracy.result())\n",
        "\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(),epcoh_accuracy.result()))\n",
        "    \n",
        "    \n",
        "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 000: Loss: 1.605, Accuracy: 75.919%\n",
            "Epoch 001: Loss: 1.613, Accuracy: 75.974%\n",
            "Epoch 002: Loss: 1.611, Accuracy: 76.108%\n",
            "Epoch 003: Loss: 1.616, Accuracy: 75.974%\n",
            "Epoch 004: Loss: 1.612, Accuracy: 75.952%\n",
            "Epoch 005: Loss: 1.607, Accuracy: 75.985%\n",
            "Epoch 006: Loss: 1.603, Accuracy: 76.598%\n",
            "Epoch 007: Loss: 1.618, Accuracy: 76.364%\n",
            "Epoch 008: Loss: 1.601, Accuracy: 76.186%\n",
            "Epoch 009: Loss: 1.605, Accuracy: 75.985%\n",
            "Duration :85.786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP7NMNegpZAy"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De894RPCpZAy"
      },
      "source": [
        "# Create a Dataset object for the test set\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
        "test_dataset = test_dataset.batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBdJYdsxpZAy"
      },
      "source": [
        "# Collect average loss and accuracy\n",
        "\n",
        "epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxHIvDJspZAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87348520-6eea-4655-d282-b188f7f5ab9d"
      },
      "source": [
        "# Loop over the test set and print scores\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value = loss(model, x, y, weight_decay)    \n",
        "    # Compute current loss\n",
        "    epoch_loss_avg(loss_value)  \n",
        "    # Compare predicted label to actual label\n",
        "    epoch_accuracy(to_categorical(y), model(x))\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
        "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.806\n",
            "Test accuracy: 70.436%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSA459JLpZAy"
      },
      "source": [
        "#### Plot the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjFbip8XpZAy"
      },
      "source": [
        "# Plot the training loss and accuracy\n",
        "\n",
        "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].plot(train_loss_results)\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B4WZ5ECpZAy"
      },
      "source": [
        "#### Predict from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY5tcWMJpZAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf9b7e5-4d9d-4adc-e480-c631d9d9f00c"
      },
      "source": [
        "# Get the model prediction for an example input\n",
        "\n",
        "predicted_label = np.argmax(model(x_train[np.newaxis,0]),axis=1)[0]\n",
        "print(\"Prediction: {}\".format(class_names[predicted_label]))\n",
        "print(\"     Label: {}\".format(class_names[train_labels[0]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: earn\n",
            "     Label: earn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPwhcW9FpZAy"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmXRC2YspZAy"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5CgD3R2pZAy"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QNeguJapZAy"
      },
      "source": [
        "# Initialize a new model\n",
        "\n",
        "model=MyModel(64,64,46)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MPkduUapZAy"
      },
      "source": [
        "#### Redefine the grad function using the @tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rsdHJawpZAy"
      },
      "source": [
        "# Use the @tf.function decorator\n",
        "@tf.function\n",
        "def grad(model, inputs, targets, wd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets, wd)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lz9XRkipZAy"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueAzDc6OpZAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026d0619-00b5-44d9-a840-59f92da1c01f"
      },
      "source": [
        "# Re-run the training loop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
        "train_dataset=train_dataset.batch(32)\n",
        "\n",
        "#keep the results for plotting\n",
        "train_loss_results=[]\n",
        "train_accuracy_results=[]\n",
        "\n",
        "num_epochs=10\n",
        "weight_decay=0.005\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss_avg=tf.keras.metrics.Mean()\n",
        "    epcoh_accuracy=tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    #training loop\n",
        "    for x,y in train_dataset:\n",
        "        #opimize the model\n",
        "        loss_value, grads=grad(model,x,y,weight_decay)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        #compute current loss\n",
        "        epoch_loss_avg(loss_value)\n",
        "        #compute predicted label to actual label\n",
        "        epcoh_accuracy(to_categorical(y),model(x))\n",
        "\n",
        "    #end epoch\n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epcoh_accuracy.result())\n",
        "\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(),epcoh_accuracy.result()))\n",
        "    \n",
        "    \n",
        "print(\"Duration :{:.3f}\".format(time.time() - start_time))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 000: Loss: 3.302, Accuracy: 50.857%\n",
            "Epoch 001: Loss: 1.886, Accuracy: 63.416%\n",
            "Epoch 002: Loss: 1.777, Accuracy: 68.370%\n",
            "Epoch 003: Loss: 1.738, Accuracy: 69.071%\n",
            "Epoch 004: Loss: 1.713, Accuracy: 70.252%\n",
            "Epoch 005: Loss: 1.696, Accuracy: 70.864%\n",
            "Epoch 006: Loss: 1.681, Accuracy: 71.599%\n",
            "Epoch 007: Loss: 1.690, Accuracy: 72.089%\n",
            "Epoch 008: Loss: 1.682, Accuracy: 72.823%\n",
            "Epoch 009: Loss: 1.662, Accuracy: 73.469%\n",
            "Duration :75.637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3adXpVEepZAy"
      },
      "source": [
        "#### Print the autograph code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERhWAmo3pZAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af0a91f-28f7-4cb6-a943-0b7ddacaac2d"
      },
      "source": [
        "# Use tf.autograph.to_code to see the generated code\n",
        "print(tf.autograph.to_code(grad.python_function))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def tf__grad(model, inputs, targets, wd):\n",
            "    with ag__.FunctionScope('grad', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
            "        do_return = False\n",
            "        retval_ = ag__.UndefinedReturnValue()\n",
            "        with ag__.ld(tf).GradientTape() as tape:\n",
            "            loss_value = ag__.converted_call(ag__.ld(loss), (ag__.ld(model), ag__.ld(inputs), ag__.ld(targets), ag__.ld(wd)), None, fscope)\n",
            "        try:\n",
            "            do_return = True\n",
            "            retval_ = (ag__.ld(loss_value), ag__.converted_call(ag__.ld(tape).gradient, (ag__.ld(loss_value), ag__.ld(model).trainable_variables), None, fscope))\n",
            "        except:\n",
            "            do_return = False\n",
            "            raise\n",
            "        return fscope.ret(retval_, do_return)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}